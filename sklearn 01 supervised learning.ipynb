{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn 01 :: supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Generalized linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Ordinary least squares (OLS)\n",
    "Minimizes the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. Risk for multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.5  0.5] \n",
      "Intercept: 2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "print('Coefficients: {} \\nIntercept: {}'.format(clf.coef_, clf.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Ridge regression\n",
    "Puts a penalty on the size of coefficients to reduce collinearity problems from OLS. Minimizes penalized residual sum of squares. Alpha value controls amount of shrinkage (higher = more shrinkage = more robust to collinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.34545455  0.34545455] \n",
      "Intercept: 0.1363636363636364\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Ridge(alpha=.5)\n",
    "clf.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "print('Coefficients: {} \\nIntercept: {}'.format(clf.coef_, clf.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000000001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "clf.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "clf.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Lasso\n",
    "The Lasso is a linear model that estimates sparse coefficients. The alpha parameter controls the degree of sparsity of the coefficients estimated. LassoCV (cross validation) and LassoLARSCV (Least Angle Regression + cross validation) available. Akaike information criterion (AIC) and the Bayes Information criterion (BIC) available: faster but less stable.\n",
    "\n",
    "The MultiTaskLasso is a linear model that estimates sparse coefficients for multiple regression problems jointly: y is a 2D array, of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha = 0.1)\n",
    "clf.fit([[0, 0], [1, 1]], [0, 1])\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 ElasticNet\n",
    "ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. \n",
    "\n",
    "Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridgeâ€™s stability under rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6. Least Angle Regression\n",
    "Least-angle regression (LARS) is a regression algorithm for high-dimensional data (when n_features >> n_samples). Computationally efficient, but sensitive to noise. LassoLARS = lasso model using lars algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
